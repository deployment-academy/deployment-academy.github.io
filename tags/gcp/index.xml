<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gcp on Deployment</title><link>https://deployment.properties/tags/gcp/</link><description>Recent content in gcp on Deployment</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Oct 2022 11:32:38 -0400</lastBuildDate><atom:link href="https://deployment.properties/tags/gcp/index.xml" rel="self" type="application/rss+xml"/><item><title>Self-managed Kubernetes with Cluster API in GCP (+ Cilium)</title><link>https://deployment.properties/posts/k8s-ops/cluster-api/</link><pubDate>Sat, 01 Oct 2022 11:32:38 -0400</pubDate><guid>https://deployment.properties/posts/k8s-ops/cluster-api/</guid><description>&lt;p>We all know the benefits of using managed Kubernetes services like GKE, EKS, AKS, etc. Given the complexity of managing the cluster infrastructure and its core components (control plane, auto-scaling, monitoring, networking, storage, etc.), using a managed Kubernetes service is generally the first choice when running workloads in production.&lt;/p>
&lt;p>However, in some situations, provisioning and managing the Kubernetes cluster from scratch might be necessary. Specific product features, security &amp;amp; compliance, costs, vendor independency, etc. are some factors that usually justify the decision to run Kubernetes by yourself. Of course, many challenges come with managing a Kubernetes cluster. I want to keep this discussion out of the scope of this tutorial since it requires special attention.&lt;/p>
&lt;p>Nowadays, when considering provision and managing a Kubernetes cluster, the tool of choice is &lt;a href="https://cluster-api.sigs.k8s.io/">Cluster API&lt;/a>. From the docs:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Cluster API is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters.&lt;/em>&lt;/p>
&lt;p>&lt;em>[&amp;hellip;] The supporting infrastructure, like virtual machines, networks, load balancers, and VPCs, as well as the Kubernetes cluster configuration are all defined in the same way that application developers operate deploying and managing their workloads. This enables consistent and repeatable cluster deployments across a wide variety of infrastructure environments.&lt;/em>&lt;/p>
&lt;/blockquote></description></item><item><title>GKE Dataplane V2 and Network Policies in Practice</title><link>https://deployment.properties/posts/devsecops/gke-dataplane-v2-network-policies/</link><pubDate>Sat, 18 Sep 2021 13:48:06 -0400</pubDate><guid>https://deployment.properties/posts/devsecops/gke-dataplane-v2-network-policies/</guid><description>&lt;p>In this tutorial, we are going to play with the &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/dataplane-v2">Google Kubernetes Engine Dataplane V2&lt;/a> and check how we can use it along with &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes Network Policies&lt;/a> to limit traffic to Pods and to obtain real-time visibility on cluster network activity.&lt;/p>
&lt;p>Dataplane V2 is a &lt;a href="https://cloud.google.com/blog/products/containers-kubernetes/bringing-ebpf-and-cilium-to-google-kubernetes-engine">recent feature in GKE&lt;/a>, with GA starting on version 1.20.6-gke.700 as of May 10, 2021. It uses &lt;a href="https://cilium.io/">Cilium&lt;/a> to process network packets in-kernel using Kubernetes-specific metadata without relying on the kube-proxy and iptables for service routing, resulting in performance improvements. Dataplane V2 brings some exciting features for cluster operations and security, such as:&lt;/p>
&lt;ul>
&lt;li>Built-in Network Policies enforcement without the need of Calico and;&lt;/li>
&lt;li>Real-time visibility, enabling cluster networking troubleshooting, auditing, and alerting.&lt;/li>
&lt;/ul></description></item><item><title>Google Cloud Endpoints in GKE with Container-native Load Balancing</title><link>https://deployment.properties/posts/devsecops/gke-cloud-endpoints/</link><pubDate>Mon, 08 Mar 2021 00:00:00 +0000</pubDate><guid>https://deployment.properties/posts/devsecops/gke-cloud-endpoints/</guid><description>&lt;p>In the source code for this tutorial, we extend the &lt;a href="https://cloud.google.com/endpoints/docs/openapi/get-started-kubernetes-engine">Getting started with Cloud Endpoints for GKE with ESP&lt;/a> documentation guide to provide an example of how to configure HTTPS between the LB and the ESP.&lt;/p></description></item><item><title>Workload Identity in Practice</title><link>https://deployment.properties/posts/devsecops/workload-identity-getting-started/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://deployment.properties/posts/devsecops/workload-identity-getting-started/</guid><description>&lt;p>In this tutorial, we&amp;rsquo;re going to go through the &lt;a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-workload-identity-better-authentication-for-your-gke-applications">Workload Identity&lt;/a> feature and see how it helps to improve the way we manage access to Google Services and APIs from applications running in &lt;a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine (GKE)&lt;/a>.&lt;/p>
&lt;p>Workload Identity is the recommended way to access Google Cloud APIs from within GKE due to its improved security properties and manageability. With Workload Identity you can control access to APIs using &lt;a href="https://cloud.google.com/iam/docs/service-accounts">Google service accounts&lt;/a> and &lt;a href="https://cloud.google.com/iam/docs/understanding-roles">IAM roles&lt;/a> without deploying static service account JSON keys to Pods and without relying on the node&amp;rsquo;s service account.&lt;/p></description></item></channel></rss>